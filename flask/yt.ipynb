{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from API\n",
    "url = \"https://datasets-server.huggingface.co/rows?dataset=breadlicker45%2Fyoutube-comments&config=default&split=train&offset=0&length=10\"\n",
    "response = requests.get(url)\n",
    "data_json = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract comments dynamically\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m comments_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m      4\u001b[0m data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{comments_key: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComment\u001b[39m\u001b[38;5;124m\"\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rows'"
     ]
    }
   ],
   "source": [
    "# Extract comments dynamically\n",
    "comments_key = list(data_json[\"rows\"][0][\"row\"].keys())[0]\n",
    "data = pd.DataFrame([row[\"row\"] for row in data_json[\"rows\"]])\n",
    "data.rename(columns={comments_key: \"Comment\"}, inplace=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/naitik/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "data1 = data.dropna(subset=[\"Comment\"])\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sentiments = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment  Positive  Negative  \\\n",
      "0  I&#39;ve read many books on Apple/Steve and ha...     0.208     0.000   \n",
      "1  Just watched this again after having watched i...     0.421     0.000   \n",
      "2                                I had one of those!     0.000     0.000   \n",
      "3      Please leave out the needless intrusive music     0.271     0.141   \n",
      "4  What about the T-mobile AMEO? Was that not bef...     0.048     0.000   \n",
      "\n",
      "   Neutral  Compound  \n",
      "0    0.792    0.7163  \n",
      "1    0.579    0.8802  \n",
      "2    1.000    0.0000  \n",
      "3    0.588    0.2732  \n",
      "4    0.952    0.5661  \n"
     ]
    }
   ],
   "source": [
    "data1[\"Positive\"] = data1[\"Comment\"].apply(lambda x: sentiments.polarity_scores(str(x))[\"pos\"])\n",
    "data1[\"Negative\"] = data1[\"Comment\"].apply(lambda x: sentiments.polarity_scores(str(x))[\"neg\"])\n",
    "data1[\"Neutral\"] = data1[\"Comment\"].apply(lambda x: sentiments.polarity_scores(str(x))[\"neu\"])\n",
    "data1['Compound'] = data1[\"Comment\"].apply(lambda x: sentiments.polarity_scores(str(x))[\"compound\"])\n",
    "print(data1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment  Positive  Negative  \\\n",
      "0  I&#39;ve read many books on Apple/Steve and ha...     0.208     0.000   \n",
      "1  Just watched this again after having watched i...     0.421     0.000   \n",
      "2                                I had one of those!     0.000     0.000   \n",
      "3      Please leave out the needless intrusive music     0.271     0.141   \n",
      "4  What about the T-mobile AMEO? Was that not bef...     0.048     0.000   \n",
      "5  i can imagine all the caos all those people th...     0.000     0.104   \n",
      "6                                               ðŸ¥°ðŸ¥°ðŸ¥°ðŸ¥°     0.000     0.000   \n",
      "7                                 Jobs is inspiring.     0.583     0.000   \n",
      "8  iPhone, that product changed my view of Steve ...     0.271     0.000   \n",
      "9  Was steve jobs gay or a macho like moi who lov...     0.408     0.000   \n",
      "\n",
      "   Neutral  Compound Sentiment  \n",
      "0    0.792    0.7163  Positive  \n",
      "1    0.579    0.8802  Positive  \n",
      "2    1.000    0.0000   Neutral  \n",
      "3    0.588    0.2732  Positive  \n",
      "4    0.952    0.5661  Positive  \n",
      "5    0.896   -0.2183  Negative  \n",
      "6    1.000    0.0000   Neutral  \n",
      "7    0.417    0.4215  Positive  \n",
      "8    0.729    0.8860  Positive  \n",
      "9    0.592    0.7351  Positive  \n"
     ]
    }
   ],
   "source": [
    "sentiment = []\n",
    "for score in data1[\"Compound\"]:\n",
    "    if score >= 0.05:\n",
    "        sentiment.append('Positive')\n",
    "    elif score <= -0.05:\n",
    "        sentiment.append('Negative')\n",
    "    else:\n",
    "        sentiment.append('Neutral')\n",
    "\n",
    "data1[\"Sentiment\"] = sentiment\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data1.drop(['Positive', 'Negative', 'Neutral', 'Compound'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/naitik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/naitik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/naitik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lzr = WordNetLemmatizer()\n",
    "\n",
    "def text_processing(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'\\n', ' ', text)  \n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "    return text  # Keep text as simple as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment Sentiment\n",
      "0  I&#39;ve read many books on Apple/Steve and ha...  Positive\n",
      "1  Just watched this again after having watched i...  Positive\n",
      "2                                I had one of those!   Neutral\n",
      "3      Please leave out the needless intrusive music  Positive\n",
      "4  What about the T-mobile AMEO? Was that not bef...  Positive\n",
      "Number of Negative Sentiments: 1\n",
      "                                            Sentence  Sentiment\n",
      "0  i39ve read many books on applesteve and have n...          2\n",
      "1  just watched this again after having watched i...          2\n",
      "2                                 i had one of those          1\n",
      "3      please leave out the needless intrusive music          2\n",
      "4  what about the tmobile ameo was that not befor...          2\n"
     ]
    }
   ],
   "source": [
    "data_copy = data2.copy()\n",
    "print(data_copy.head())\n",
    "data_copy['Comment'] = data_copy['Comment'].astype(str).apply(text_processing)\n",
    "\n",
    "le = LabelEncoder()\n",
    "data_copy['Sentiment'] = le.fit_transform(data_copy['Sentiment'])\n",
    "\n",
    "processed_data = pd.DataFrame({\n",
    "    'Sentence': data_copy['Comment'],\n",
    "    'Sentiment': data_copy['Sentiment']\n",
    "})\n",
    "\n",
    "negative_count = (processed_data['Sentiment'] == le.transform(['Negative'])[0]).sum()\n",
    "print(\"Number of Negative Sentiments:\", negative_count)\n",
    "\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417, 116) (417,)\n"
     ]
    }
   ],
   "source": [
    "df_negative = processed_data[processed_data['Sentiment'] == 0]\n",
    "df_neutral = processed_data[processed_data['Sentiment'] == 1]\n",
    "df_positive = processed_data[processed_data['Sentiment'] == 2]\n",
    "\n",
    "df_negative_upsampled = resample(df_negative, replace=True, n_samples=205, random_state=42)\n",
    "df_neutral_upsampled = resample(df_neutral, replace=True, n_samples=205, random_state=42)\n",
    "final_data = pd.concat([df_negative_upsampled, df_neutral_upsampled, df_positive])\n",
    "\n",
    "corpus = final_data['Sentence'].tolist()\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = final_data['Sentiment'].values\n",
    "print(X.shape, y.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
