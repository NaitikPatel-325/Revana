{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Import functions for data preprocessing & data preparation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('../input/blackadam-trailer-comments/comments.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data1 = data.drop(['Unnamed: 0', 'Likes', 'Time', 'user', 'UserLink'], axis=1)\n",
    "\n",
    "# Download NLTK VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply sentiment analysis\n",
    "data1[\"Positive\"] = data1[\"Comment\"].apply(lambda x: sentiments.polarity_scores(str(x))[\"pos\"])\n",
    "data1[\"Negative\"] = data1[\"Comment\"].apply(lambda x: sentiments.polarity_scores(str(x))[\"neg\"])\n",
    "data1[\"Neutral\"] = data1[\"Comment\"].apply(lambda x: sentiments.polarity_scores(str(x))[\"neu\"])\n",
    "data1['Compound'] = data1[\"Comment\"].apply(lambda x: sentiments.polarity_scores(str(x))[\"compound\"])\n",
    "\n",
    "# Assign sentiment labels\n",
    "sentiment = []\n",
    "for score in data1[\"Compound\"]:\n",
    "    if score >= 0.05:\n",
    "        sentiment.append('Positive')\n",
    "    elif score <= -0.05:\n",
    "        sentiment.append('Negative')\n",
    "    else:\n",
    "        sentiment.append('Neutral')\n",
    "\n",
    "data1[\"Sentiment\"] = sentiment\n",
    "\n",
    "# Drop sentiment score columns\n",
    "data2 = data1.drop(['Positive', 'Negative', 'Neutral', 'Compound'], axis=1)\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize text processing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lzr = WordNetLemmatizer()\n",
    "\n",
    "def text_processing(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\n', ' ', text)  # Remove new lines\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove multiple spaces\n",
    "    text = ' '.join([lzr.lemmatize(word) for word in word_tokenize(text) if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply text preprocessing\n",
    "data_copy = data2.copy()\n",
    "data_copy['Comment'] = data_copy['Comment'].astype(str).apply(text_processing)\n",
    "\n",
    "# Encode sentiment labels\n",
    "le = LabelEncoder()\n",
    "data_copy['Sentiment'] = le.fit_transform(data_copy['Sentiment'])\n",
    "\n",
    "# Create processed dataset\n",
    "processed_data = pd.DataFrame({\n",
    "    'Sentence': data_copy['Comment'],\n",
    "    'Sentiment': data_copy['Sentiment']\n",
    "})\n",
    "\n",
    "# Balance dataset using upsampling\n",
    "df_negative = processed_data[processed_data['Sentiment'] == 0]\n",
    "df_neutral = processed_data[processed_data['Sentiment'] == 1]\n",
    "df_positive = processed_data[processed_data['Sentiment'] == 2]\n",
    "\n",
    "df_negative_upsampled = resample(df_negative, replace=True, n_samples=205, random_state=42)\n",
    "df_neutral_upsampled = resample(df_neutral, replace=True, n_samples=205, random_state=42)\n",
    "final_data = pd.concat([df_negative_upsampled, df_neutral_upsampled, df_positive])\n",
    "\n",
    "# Convert text data to numerical features\n",
    "corpus = final_data['Sentence'].tolist()\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = final_data['Sentiment'].values\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "nb_score = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', nb_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
