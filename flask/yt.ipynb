{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/naitik/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/naitik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/naitik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/naitik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "file_path = \"amazon_vfl_reviews.xls\"  \n",
    "\n",
    "def text_processing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.10:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@app.route('/api/v1/youtube-comments', methods=['POST'])\n",
    "def process_comments():\n",
    "    data = request.json\n",
    "    comments = data.get(\"comments\", [])\n",
    "\n",
    "    if not comments:\n",
    "        return jsonify({'error': 'No comments provided'}), 400\n",
    "\n",
    "    df = pd.DataFrame(comments, columns=[\"Comment\"])\n",
    "    df = df.dropna(subset=[\"Comment\"])\n",
    "    df['Comment'] = df['Comment'].astype(str).apply(text_processing)\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    df[\"Compound\"] = df[\"Comment\"].apply(lambda x: sentiments.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "    # Classify Sentiment\n",
    "    df[\"Sentiment\"] = df[\"Compound\"].apply(lambda x: 'Positive' if x >= 0.05 else 'Negative' if x <= -0.05 else 'Neutral')\n",
    "\n",
    "    # Encode Sentiments\n",
    "    le = LabelEncoder()\n",
    "    df['Sentiment'] = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "    processed_comments = df[['Comment', 'Sentiment']].to_dict(orient='records')\n",
    "    return jsonify({'comments': processed_comments})\n",
    "\n",
    "\n",
    "@app.route('/api/v1/amazon-reviews', methods=['GET'])\n",
    "def get_reviews_by_asin():\n",
    "    asin = request.args.get('asin')\n",
    "\n",
    "    if not asin:\n",
    "        return jsonify({\"error\": \"ASIN parameter is required\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "\n",
    "        # Check if ASIN column exists\n",
    "        asin_column = next((col for col in df.columns if \"asin\" in col.lower()), None)\n",
    "\n",
    "        if not asin_column:\n",
    "            return jsonify({\"error\": \"ASIN column not found\"}), 400\n",
    "\n",
    "        # Filter data by ASIN\n",
    "        filtered_df = df[df[asin_column] == asin]\n",
    "\n",
    "        if filtered_df.empty:\n",
    "            return jsonify({\"error\": \"No data found for this ASIN\"}), 404\n",
    "\n",
    "        # Return data\n",
    "        output = filtered_df.to_dict(orient=\"records\")\n",
    "        return jsonify(output)\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_analyzer.pkl', 'wb') as file:\n",
    "    pickle.dump(sentiments, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
